{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45dfffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial packages imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Data handling and numerical computations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning packages\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Machine learning models (Basic)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Machine learning models (Advanced)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Real-time data and deployment (for future use)\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Ignoring warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "print(\"Initial packages imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de398e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged_data:\n",
      "['station_key', 'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq', 'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday', 'daily_total', 'hour_00', 'hour_01', 'hour_02', 'hour_03', 'hour_04', 'hour_05', 'hour_06', 'hour_07', 'hour_08', 'hour_09', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'road_functional_hierarchy', 'lane_count', 'road_classification_type', 'suburb', 'wgs84_latitude', 'wgs84_longitude']\n",
      "\n",
      "Are expected station metadata columns present?\n",
      "suburb: âœ…\n",
      "road_functional_hierarchy: âœ…\n",
      "road_classification_type: âœ…\n",
      "lane_count: âœ…\n",
      "wgs84_latitude: âœ…\n",
      "wgs84_longitude: âœ…\n",
      "\n",
      "Number of rows with completely missing station metadata: 0\n",
      "\n",
      "Sample merged rows with metadata:\n",
      "   station_key             suburb road_functional_hierarchy  \\\n",
      "0     15934005  Twelve Mile Creek              Primary Road   \n",
      "1     15934004  Twelve Mile Creek              Primary Road   \n",
      "2     15934005  Twelve Mile Creek              Primary Road   \n",
      "3        57052  Constitution Hill              Primary Road   \n",
      "4     15934004  Twelve Mile Creek              Primary Road   \n",
      "\n",
      "  road_classification_type      lane_count  wgs84_latitude  wgs84_longitude  \n",
      "0                  Highway  TwoOrMoreLanes      -32.656689       151.850708  \n",
      "1                  Highway  TwoOrMoreLanes      -32.656384       151.851227  \n",
      "2                  Highway  TwoOrMoreLanes      -32.656689       151.850708  \n",
      "3                    Drive  TwoOrMoreLanes      -33.797592       150.975433  \n",
      "4                  Highway  TwoOrMoreLanes      -32.656384       151.851227  \n"
     ]
    }
   ],
   "source": [
    "# Select only essential metadata columns from the station file (with correct names)\n",
    "selected_station_cols = [\n",
    "    'station_key',\n",
    "    'suburb',\n",
    "    'road_functional_hierarchy',\n",
    "    'road_classification_type',\n",
    "    'lane_count',\n",
    "    'wgs84_latitude',\n",
    "    'wgs84_longitude'\n",
    "]\n",
    "\n",
    "\n",
    "# Load station metadata (small file, selective columns)\n",
    "station_data = pd.read_csv(\n",
    "    \"datasets_cleaned/trafficStations.csv\",\n",
    "    usecols=selected_station_cols\n",
    ")\n",
    "\n",
    "# Prepare for chunked loading\n",
    "chunks = []\n",
    "chunk_size = 25000  # adjust based on RAM\n",
    "\n",
    "# Stream traffic data in chunks\n",
    "for chunk in pd.read_csv(\n",
    "    \"datasets_cleaned/trafficData.csv\",\n",
    "    chunksize=chunk_size,\n",
    "    low_memory=True\n",
    "):\n",
    "    merged_chunk = pd.merge(chunk, station_data, on='station_key', how='left')\n",
    "    chunks.append(merged_chunk)\n",
    "\n",
    "# Combine chunks\n",
    "merged_data = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Convert core data types after merge to reduce memory\n",
    "column_types = {\n",
    "    'station_key': 'int32',\n",
    "    'traffic_direction_seq': 'int8',\n",
    "    'cardinal_direction_seq': 'int8',\n",
    "    'classification_seq': 'int8',\n",
    "    'year': 'int16',\n",
    "    'month': 'int8',\n",
    "    'day': 'int8',\n",
    "    'day_of_week': 'int8',\n",
    "    'public_holiday': 'bool',\n",
    "    'school_holiday': 'bool',\n",
    "    'daily_total': 'int32'\n",
    "}\n",
    "for i in range(24):\n",
    "    column_types[f'hour_{i:02d}'] = 'int16'\n",
    "\n",
    "merged_data = merged_data.astype({k: v for k, v in column_types.items() if k in merged_data.columns})\n",
    "\n",
    "# --- Sanity Check Section ---\n",
    "print(\"Columns in merged_data:\")\n",
    "print(merged_data.columns.tolist())\n",
    "\n",
    "expected_cols = selected_station_cols[1:]  # Skip station_key (already known to exist)\n",
    "\n",
    "print(\"\\nAre expected station metadata columns present?\")\n",
    "for col in expected_cols:\n",
    "    print(f\"{col}: {'âœ…' if col in merged_data.columns else 'âŒ'}\")\n",
    "\n",
    "null_check = merged_data[expected_cols].isnull().all(axis=1).sum()\n",
    "print(f\"\\nNumber of rows with completely missing station metadata: {null_check}\")\n",
    "\n",
    "print(\"\\nSample merged rows with metadata:\")\n",
    "print(merged_data[['station_key'] + expected_cols].dropna().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8929c778",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Re-import libraries after kernel reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simulate the structure of merged_data\n",
    "np.random.seed(42)\n",
    "merged_data = pd.DataFrame({\n",
    "    'congested': np.random.choice([0, 1], 1000)\n",
    "})\n",
    "for i in range(24):\n",
    "    merged_data[f'hour_{i:02d}'] = np.random.randint(0, 300, 1000)\n",
    "\n",
    "# Step 1: Define hour columns\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "\n",
    "# Step 2: Drop rows with missing values\n",
    "merged_data = merged_data.dropna(subset=hour_cols + ['congested'])\n",
    "\n",
    "# Step 3: Reshape data\n",
    "X_seq = merged_data[hour_cols].values\n",
    "X_lstm = X_seq.reshape((X_seq.shape[0], 24, 1))\n",
    "y_lstm = merged_data['congested'].astype(int).values\n",
    "\n",
    "# Step 4: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, stratify=y_lstm, random_state=42)\n",
    "\n",
    "# Step 5: Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(24, 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Step 7: Predict and evaluate\n",
    "y_pred_probs = model.predict(X_test).flatten()\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Step 8: Metrics\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report_dict = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred)\n",
    "}\n",
    "report_df = pd.DataFrame(report_dict, index=[\"Score\"]).T\n",
    "\n",
    "print(\"\\nðŸ“Š LSTM Model Performance:\")\n",
    "print(report_df.to_string())\n",
    "\n",
    "# Step 9: Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Not Congested', 'Congested'],\n",
    "            yticklabels=['Not Congested', 'Congested'])\n",
    "plt.title(\"LSTM Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
