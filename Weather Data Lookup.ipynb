{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d81d332",
   "metadata": {},
   "source": [
    "# **Weather Data Lookup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3316f95",
   "metadata": {},
   "source": [
    "1. Read trafficData.csv, trafficStations.csv, and weatherStations.csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0462a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read each CSV file into its own DataFrame\n",
    "trafficData = pd.read_csv('datasets_cleaned/trafficData.csv')\n",
    "trafficStations = pd.read_csv('datasets_cleaned/trafficStations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae0469",
   "metadata": {},
   "source": [
    "2. Merge station coordinates information with traffic data measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7a1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary columns in trafficData to avoid excess memory use\n",
    "trafficData = trafficData[['station_key', 'year', 'month', 'day',]]\n",
    "\n",
    "# keep only necessary columns in trafficStations to avoid excess memory use\n",
    "stationLocations = trafficStations[['station_key', 'wgs84_latitude', 'wgs84_longitude']]\n",
    "\n",
    "# merge the latitude and longitude info into trafficData\n",
    "trafficData = trafficData.merge(stationLocations, on='station_key', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af8c2c",
   "metadata": {},
   "source": [
    "3. Round latitude and longitude values to the nearest 0.05 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05590b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round latitude and create a new column\n",
    "trafficData['lat_rounded'] = (trafficData['wgs84_latitude'] / 0.05).round() * 0.05\n",
    "# round longitude and create a new column\n",
    "trafficData['lon_rounded'] = (trafficData['wgs84_longitude'] / 0.05).round() * 0.05\n",
    "\n",
    "# drop the original latitude and longitude columns\n",
    "trafficData = trafficData.drop(columns=['wgs84_latitude', 'wgs84_longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3e74f",
   "metadata": {},
   "source": [
    "4. Convert year, month, and day columns to a single datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ece8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column\n",
    "trafficData['date'] = pd.to_datetime(trafficData[['year', 'month', 'day']])\n",
    "\n",
    "# drop the original year/month/day columns\n",
    "trafficData.drop(columns=['year', 'month', 'day'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f0377",
   "metadata": {},
   "source": [
    "5. Group rows by coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc78dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates just for grouping purposes (we’ll keep the full df later)\n",
    "unique_requests = trafficData[['lat_rounded', 'lon_rounded', 'date']].drop_duplicates()\n",
    "\n",
    "# group by location\n",
    "grouped = unique_requests.groupby(['lat_rounded', 'lon_rounded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82ca46",
   "metadata": {},
   "source": [
    "6. Determine all unique API requests to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583fb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_requests = []\n",
    "\n",
    "for (lat, lon), group in grouped:\n",
    "    group = group.sort_values('date')\n",
    "    # Identify breaks in date sequence (where date difference > 1 day)\n",
    "    diff = group['date'].diff().dt.days\n",
    "    range_id = (diff != 1).cumsum()\n",
    "\n",
    "    for _, sub in group.groupby(range_id):\n",
    "        start_date = sub['date'].iloc[0]\n",
    "        end_date = sub['date'].iloc[-1]\n",
    "        api_requests.append({\n",
    "            'lat_rounded': lat,\n",
    "            'lon_rounded': lon,\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date\n",
    "        })\n",
    "\n",
    "# Create a DataFrame of API requests\n",
    "request_df = pd.DataFrame(api_requests)\n",
    "\n",
    "# format dates as YYYYMMDD integers\n",
    "request_df['start_date'] = request_df['start_date'].dt.strftime('%Y%m%d').astype(int)\n",
    "request_df['end_date'] = request_df['end_date'].dt.strftime('%Y%m%d').astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c3490",
   "metadata": {},
   "source": [
    "7. Create function to retrieve API response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af25d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather_data(start, finish, lat_rounded, lon_rounded):\n",
    "\n",
    "    # construct the URL\n",
    "    url = (\n",
    "        \"https://www.longpaddock.qld.gov.au/cgi-bin/silo/DataDrillDataset.php\"\n",
    "        f\"?lat={lat_rounded}&lon={lon_rounded}&start={start}&finish={finish}&format=json&comment=RXN&username=danielalexanderchung@outlook.com&password=apirequest\"\n",
    "    )\n",
    "\n",
    "    # make the request\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # parse JSON response\n",
    "    data = response.json()\n",
    "    daily_data = data.get(\"data\", [])\n",
    "\n",
    "   # Extract weather data for each date in the range\n",
    "    weather_list = []\n",
    "    for row in daily_data:\n",
    "        variables = {var['variable_code']: var['value'] for var in row['variables']}\n",
    "        weather_list.append({\n",
    "            'date': row.get(\"date\"),\n",
    "            'lat_rounded': lat_rounded,\n",
    "            'lon_rounded': lon_rounded,\n",
    "            'daily_rain': variables.get(\"daily_rain\"),\n",
    "            'max_temp': variables.get(\"max_temp\"),\n",
    "            'min_temp': variables.get(\"min_temp\"),\n",
    "        })\n",
    "\n",
    "    return weather_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe11cd",
   "metadata": {},
   "source": [
    "8. Collect all API responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049063b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 29080/29080 requests."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# -- Thread-safe wrapper for one row of the DataFrame --\n",
    "def fetch_row_data(row):\n",
    "    lat = row['lat_rounded']\n",
    "    lon = row['lon_rounded']\n",
    "    start = str(int(row['start_date']))\n",
    "    end = str(int(row['end_date']))\n",
    "    try:\n",
    "        return get_weather_data(start, end, lat, lon)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed request for {lat}, {lon} from {start} to {end}: {e}\")\n",
    "        return []\n",
    "\n",
    "# -- Parallel Execution --\n",
    "all_API_responses = []\n",
    "\n",
    "max_workers = 10  # Try 5–10; increase carefully based on system/network\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(fetch_row_data, row) for _, row in request_df.iterrows()]\n",
    "\n",
    "    for idx, future in enumerate(as_completed(futures), 1):\n",
    "        all_API_responses.extend(future.result())\n",
    "        sys.stdout.write(f\"\\rProcessed {idx}/{len(futures)} requests.\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# Convert to DataFrame\n",
    "weatherData = pd.DataFrame(all_API_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47bd966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date to datetime format for merging\n",
    "trafficData['date'] = pd.to_datetime(trafficData['date'])\n",
    "weatherData['date'] = pd.to_datetime(weatherData['date'])\n",
    "\n",
    "# merge weather onto trafficData using lat, lon, and date\n",
    "weatherData_merged = trafficData.merge(\n",
    "    weatherData[['date', 'lat_rounded', 'lon_rounded', 'daily_rain', 'max_temp', 'min_temp']],\n",
    "    how='left',\n",
    "    on=['lat_rounded', 'lon_rounded', 'date']\n",
    ")\n",
    "\n",
    "# remove station_key column\n",
    "weatherData_merged = weatherData_merged[['lat_rounded', 'lon_rounded', 'date', 'daily_rain', 'max_temp', 'min_temp']]\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "weatherData_merged.to_csv(\"datasets_cleaned/weatherData.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
