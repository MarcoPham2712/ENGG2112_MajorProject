{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9591c138",
   "metadata": {},
   "source": [
    "# **Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f9d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843948c",
   "metadata": {},
   "source": [
    "### **List of Traffic Data Collection Stations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb5544",
   "metadata": {},
   "source": [
    "1. Read the .csv file and store in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ee54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficStations = pd.read_csv('datasets/road_traffic_counts_station_reference.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da351768",
   "metadata": {},
   "source": [
    "2. Drop unnecesary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b08804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'the_geom', 'the_geom_webmercator', 'cartodb_id', 'record_id', 'station_id', 'road_number', 'link_number', 'road_name', 'common_road_name', 'secondary_name', 'road_name_base', 'road_name_type', 'mab_identifier', 'road_classification_admin', 'lambert_easting', 'lambert_northing', 'publish', 'md5', 'updated_on', 'device_type'\n",
    "trafficStations = trafficStations.drop(columns=['the_geom','the_geom_webmercator','cartodb_id','record_id','station_id','road_number','link_number','road_name','common_road_name','secondary_name','road_name_base','road_name_type','mab_identifier','road_classification_admin','lambert_easting','lambert_northing','publish','md5', 'updated_on', 'device_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb64c0",
   "metadata": {},
   "source": [
    "3. Manually fill in the rms_region, lga, suburb, and post_code columns for the two entries with NULL values.  \n",
    "*(The NULL values occurred because the stations are located on the boundary between two postcodes. I used their coordinates in Google Maps to determine an appropriate postcode for each.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5457791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating row at index 589\n",
    "trafficStations.loc[589, ['rms_region', 'lga', 'suburb', 'post_code']] = ['Sydney', 'Sydney', 'Chippendale', 2008]\n",
    "\n",
    "# updating row at index 1668\n",
    "trafficStations.loc[1668, ['rms_region', 'lga', 'suburb', 'post_code']] = ['Sydney', 'The Hills', 'Baulkham Hills', 2153]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72787e",
   "metadata": {},
   "source": [
    "4. Store cleaned DataFrame into a new .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bd7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficStations.to_csv('datasets_cleaned/trafficStations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4581da3",
   "metadata": {},
   "source": [
    "### **Historical Traffic Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ef70c",
   "metadata": {},
   "source": [
    "1. Read the .csv files and store in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a95a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read each CSV file into its own DataFrame\n",
    "df0 = pd.read_csv('datasets/road_traffic_counts_hourly_permanent0.csv')\n",
    "df1 = pd.read_csv('datasets/road_traffic_counts_hourly_permanent1.csv', low_memory=False) # low_memory=False to avoid dtype warning\n",
    "df2 = pd.read_csv('datasets/road_traffic_counts_hourly_permanent2.csv')\n",
    "df3 = pd.read_csv('datasets/road_traffic_counts_hourly_permanent3.csv')\n",
    "\n",
    "# concatenate the DataFrames into a single DataFrame\n",
    "trafficData = pd.concat([df0, df1, df2, df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75feca",
   "metadata": {},
   "source": [
    "2. Extract the day from the date column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edaef486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the day from the date column\n",
    "trafficData['date'] = pd.to_datetime(trafficData['date'])\n",
    "trafficData['day'] = trafficData['date'].dt.day\n",
    "\n",
    "# get the list of columns\n",
    "cols = list(trafficData.columns)\n",
    "\n",
    "# find the index of the month column and insert the day column right after\n",
    "month_idx = cols.index('month')\n",
    "cols.insert(month_idx + 1, cols.pop(cols.index('day')))\n",
    "\n",
    "# reorder DataFrame\n",
    "trafficData = trafficData[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2dcf55",
   "metadata": {},
   "source": [
    "3. Drop unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c182707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'the_geom', 'the_geom_webmercator', 'cartodb_id', 'record_id', 'updated_on', 'md5', and 'date' columns\n",
    "trafficData = trafficData.drop(columns=['the_geom','the_geom_webmercator','cartodb_id','record_id','updated_on','md5', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf2a90",
   "metadata": {},
   "source": [
    "4. Replace NaN values with 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cbc0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficData.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43a608",
   "metadata": {},
   "source": [
    "5. Convert traffic volume count columns to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eef528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns with indices 11 to 34 (HOUR_00 to HOUR_23 columns)\n",
    "cols_to_convert = trafficData.columns[11:35]\n",
    "\n",
    "# Convert the selected columns to integers\n",
    "trafficData[cols_to_convert] = trafficData[cols_to_convert].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ec34f",
   "metadata": {},
   "source": [
    "6. There are some station keys included in trafficData.csv that are not included in trafficStations.csv because of missing metadata. The total number of rows with station keys in trafficData.csv that do not match up with the station key in trafficStations.csv is 357228. As a percentage of the total length of trafficData.csv, this represents 9.35% of the total traffic data records. These rows will be removed, since the metadata cannot be left as NaN or manually filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "637104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only rows where station_key exists in trafficStations\n",
    "trafficData = trafficData[trafficData['station_key'].isin(trafficStations['station_key'])]\n",
    "\n",
    "# reset the index\n",
    "trafficData = trafficData.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d3074",
   "metadata": {},
   "source": [
    "7. Store cleaned DataFrame into a new .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7444bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficData.to_csv('datasets_cleaned/trafficData.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
