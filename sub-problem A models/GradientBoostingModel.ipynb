{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b16137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial packages imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Data handling and numerical computations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning packages\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Machine learning models (Basic)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Machine learning models (Advanced)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Real-time data and deployment (for future use)\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Ignoring warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "print(\"Initial packages imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d885eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged_data:\n",
      "['station_key', 'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq', 'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday', 'daily_total', 'hour_00', 'hour_01', 'hour_02', 'hour_03', 'hour_04', 'hour_05', 'hour_06', 'hour_07', 'hour_08', 'hour_09', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'road_functional_hierarchy', 'lane_count', 'road_classification_type', 'suburb', 'wgs84_latitude', 'wgs84_longitude']\n",
      "\n",
      "Are expected station metadata columns present?\n",
      "suburb: âœ…\n",
      "road_functional_hierarchy: âœ…\n",
      "road_classification_type: âœ…\n",
      "lane_count: âœ…\n",
      "wgs84_latitude: âœ…\n",
      "wgs84_longitude: âœ…\n",
      "\n",
      "Number of rows with completely missing station metadata: 0\n",
      "\n",
      "Sample merged rows with metadata:\n",
      "   station_key             suburb road_functional_hierarchy  \\\n",
      "0     15934005  Twelve Mile Creek              Primary Road   \n",
      "1     15934004  Twelve Mile Creek              Primary Road   \n",
      "2     15934005  Twelve Mile Creek              Primary Road   \n",
      "3        57052  Constitution Hill              Primary Road   \n",
      "4     15934004  Twelve Mile Creek              Primary Road   \n",
      "\n",
      "  road_classification_type      lane_count  wgs84_latitude  wgs84_longitude  \n",
      "0                  Highway  TwoOrMoreLanes      -32.656689       151.850708  \n",
      "1                  Highway  TwoOrMoreLanes      -32.656384       151.851227  \n",
      "2                  Highway  TwoOrMoreLanes      -32.656689       151.850708  \n",
      "3                    Drive  TwoOrMoreLanes      -33.797592       150.975433  \n",
      "4                  Highway  TwoOrMoreLanes      -32.656384       151.851227  \n"
     ]
    }
   ],
   "source": [
    "# Select only essential metadata columns from the station file (with correct names)\n",
    "selected_station_cols = [\n",
    "    'station_key',\n",
    "    'suburb',\n",
    "    'road_functional_hierarchy',\n",
    "    'road_classification_type',\n",
    "    'lane_count',\n",
    "    'wgs84_latitude',\n",
    "    'wgs84_longitude'\n",
    "]\n",
    "\n",
    "\n",
    "# Load station metadata (small file, selective columns)\n",
    "station_data = pd.read_csv(\n",
    "    \"datasets_cleaned/trafficStations.csv\",\n",
    "    usecols=selected_station_cols\n",
    ")\n",
    "\n",
    "# Prepare for chunked loading\n",
    "chunks = []\n",
    "chunk_size = 25000  # adjust based on RAM\n",
    "\n",
    "# Stream traffic data in chunks\n",
    "for chunk in pd.read_csv(\n",
    "    \"datasets_cleaned/trafficData.csv\",\n",
    "    chunksize=chunk_size,\n",
    "    low_memory=True\n",
    "):\n",
    "    merged_chunk = pd.merge(chunk, station_data, on='station_key', how='left')\n",
    "    chunks.append(merged_chunk)\n",
    "\n",
    "# Combine chunks\n",
    "merged_data = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Convert core data types after merge to reduce memory\n",
    "column_types = {\n",
    "    'station_key': 'int32',\n",
    "    'traffic_direction_seq': 'int8',\n",
    "    'cardinal_direction_seq': 'int8',\n",
    "    'classification_seq': 'int8',\n",
    "    'year': 'int16',\n",
    "    'month': 'int8',\n",
    "    'day': 'int8',\n",
    "    'day_of_week': 'int8',\n",
    "    'public_holiday': 'bool',\n",
    "    'school_holiday': 'bool',\n",
    "    'daily_total': 'int32'\n",
    "}\n",
    "for i in range(24):\n",
    "    column_types[f'hour_{i:02d}'] = 'int16'\n",
    "\n",
    "merged_data = merged_data.astype({k: v for k, v in column_types.items() if k in merged_data.columns})\n",
    "\n",
    "# --- Sanity Check Section ---\n",
    "print(\"Columns in merged_data:\")\n",
    "print(merged_data.columns.tolist())\n",
    "\n",
    "expected_cols = selected_station_cols[1:]  # Skip station_key (already known to exist)\n",
    "\n",
    "print(\"\\nAre expected station metadata columns present?\")\n",
    "for col in expected_cols:\n",
    "    print(f\"{col}: {'âœ…' if col in merged_data.columns else 'âŒ'}\")\n",
    "\n",
    "null_check = merged_data[expected_cols].isnull().all(axis=1).sum()\n",
    "print(f\"\\nNumber of rows with completely missing station metadata: {null_check}\")\n",
    "\n",
    "print(\"\\nSample merged rows with metadata:\")\n",
    "print(merged_data[['station_key'] + expected_cols].dropna().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1c6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_label_leakage(df, target_col='congested', threshold=0.95):\n",
    "    \"\"\"\n",
    "    Detect potential label leakage by:\n",
    "    - Computing correlation with the target\n",
    "    - Detecting engineered rules like: (col > t) == target\n",
    "    \"\"\"\n",
    "    suspicious = []\n",
    "\n",
    "    # Correlation-based check\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    if target_col not in numeric_df.columns:\n",
    "        raise ValueError(f\"'{target_col}' column not found in numeric columns.\")\n",
    "\n",
    "    corrs = numeric_df.corr()[target_col].drop(target_col)\n",
    "    high_corr = corrs[abs(corrs) > threshold]\n",
    "    if not high_corr.empty:\n",
    "        print(\"âš ï¸ Potential leakage via correlation:\")\n",
    "        print(high_corr)\n",
    "\n",
    "    # Rule-based check\n",
    "    print(\"\\nðŸ” Testing binary rules (col > threshold == label):\")\n",
    "    for col in numeric_df.columns:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        series = df[col]\n",
    "        for p in [25, 50, 75]:\n",
    "            t = np.percentile(series.dropna(), p)\n",
    "            rule_match = (series > t).astype(int) == df[target_col]\n",
    "            if rule_match.all():\n",
    "                print(f\"ðŸš¨ {col} > {t:.1f} perfectly splits target â†’ LEAKAGE\")\n",
    "                suspicious.append(col)\n",
    "                break\n",
    "\n",
    "    return suspicious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13d3fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_columns_in_data': [],\n",
       " 'feature_cols_count': 39,\n",
       " 'sample_feature_cols': ['traffic_direction_seq',\n",
       "  'cardinal_direction_seq',\n",
       "  'classification_seq',\n",
       "  'year',\n",
       "  'month',\n",
       "  '...',\n",
       "  'hour_22',\n",
       "  'hour_23',\n",
       "  'road_functional_hierarchy',\n",
       "  'road_classification_type',\n",
       "  'suburb']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Sanity check that all required columns exist in the dataset\n",
    "expected_columns = [\n",
    "    'station_key', 'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq',\n",
    "    'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday', 'daily_total'\n",
    "] + [f'hour_{i:02d}' for i in range(24)] + [\n",
    "    'road_functional_hierarchy', 'lane_count', 'road_classification_type', \n",
    "    'suburb', 'wgs84_latitude', 'wgs84_longitude'\n",
    "]\n",
    "\n",
    "# Try to list columns in merged_data (if loaded)\n",
    "try:\n",
    "    dataset_columns = merged_data.columns.tolist()\n",
    "    missing_columns = [col for col in expected_columns if col not in dataset_columns]\n",
    "except Exception:\n",
    "    missing_columns = expected_columns  # fallback\n",
    "\n",
    "# Step 2: Redefine component feature groups\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "categorical_features = ['road_functional_hierarchy', 'road_classification_type', 'suburb']\n",
    "numeric_meta = [\n",
    "    'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq',\n",
    "    'year', 'month', 'day', 'day_of_week',\n",
    "    'public_holiday', 'school_holiday', 'lane_count',\n",
    "    'wgs84_latitude', 'wgs84_longitude'\n",
    "]\n",
    "\n",
    "# Step 3: Combine final feature set\n",
    "feature_cols = numeric_meta + hour_cols + categorical_features\n",
    "\n",
    "# Output result\n",
    "{\n",
    "    \"missing_columns_in_data\": missing_columns,\n",
    "    \"feature_cols_count\": len(feature_cols),\n",
    "    \"sample_feature_cols\": feature_cols[:5] + [\"...\"] + feature_cols[-5:]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a65380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Columns with remaining NaNs (should all be 0):\n",
      "Series([], dtype: int64)\n",
      "âœ… Final shape of training data: (2727247, 39), test data: (681812, 39)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert lane_count safely\n",
    "merged_data['lane_count'] = pd.to_numeric(merged_data['lane_count'], errors='coerce')\n",
    "\n",
    "# Step 2: Fill missing values intelligently\n",
    "# You can customize these defaults based on your domain knowledge\n",
    "merged_data['lane_count'] = merged_data['lane_count'].fillna(2)\n",
    "merged_data['wgs84_latitude'] = merged_data['wgs84_latitude'].fillna(merged_data['wgs84_latitude'].median())\n",
    "merged_data['wgs84_longitude'] = merged_data['wgs84_longitude'].fillna(merged_data['wgs84_longitude'].median())\n",
    "\n",
    "# Step 3: Confirm hour columns and fill any NaNs with 0 (assuming no traffic recorded)\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "merged_data[hour_cols] = merged_data[hour_cols].fillna(0)\n",
    "\n",
    "# Define binary target label before feature validation\n",
    "merged_data['congested'] = (merged_data['daily_total'] > 1000).astype(int)\n",
    "merged_data = merged_data.dropna(subset=['road_classification_type'])\n",
    "\n",
    "# Step 4: Confirm that all required columns are now clean\n",
    "columns_used = numeric_meta + hour_cols + categorical_features + ['congested']\n",
    "missing_check = merged_data[columns_used].isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Diagnostic print\n",
    "print(\"ðŸ” Columns with remaining NaNs (should all be 0):\")\n",
    "print(missing_check[missing_check > 0])\n",
    "\n",
    "# Now proceed safely\n",
    "X = merged_data[numeric_meta + hour_cols + categorical_features]\n",
    "y = merged_data['congested']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"âœ… Final shape of training data: {X_train.shape}, test data: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Identify categorical columns\n",
    "categorical_features = ['road_functional_hierarchy', 'road_classification_type', 'suburb']\n",
    "\n",
    "# Step 2: One-hot encode them (drop_first=True to reduce multicollinearity)\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Step 3: Fit XGBoost model with encoded data\n",
    "model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_encoded, y)\n",
    "\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Ensure columns match (align test to train)\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_encoded, y_train)\n",
    "y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "# Step 3: Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 4: Format and return results\n",
    "report_df = pd.DataFrame({\n",
    "    \"Score\": [acc, prec, rec, f1]\n",
    "}, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Not Congested', 'Congested'],\n",
    "            yticklabels=['Not Congested', 'Congested'])\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Assume merged_data is already loaded and cleaned, with column 'congested' defined ---\n",
    "\n",
    "# 1. Group infrequent suburbs into 'Other'\n",
    "suburb_counts = merged_data['suburb'].value_counts()\n",
    "common_suburbs = suburb_counts[suburb_counts > 50].index\n",
    "merged_data['suburb'] = merged_data['suburb'].apply(lambda x: x if x in common_suburbs else 'Other')\n",
    "\n",
    "# 2. Add peak-hour aggregates\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "merged_data['morning_peak'] = merged_data[[f'hour_{i:02d}' for i in range(7, 10)]].sum(axis=1)\n",
    "merged_data['evening_peak'] = merged_data[[f'hour_{i:02d}' for i in range(16, 19)]].sum(axis=1)\n",
    "\n",
    "# 3. Define feature columns\n",
    "categorical_features = ['road_functional_hierarchy', 'road_classification_type', 'suburb']\n",
    "numeric_meta = ['traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq',\n",
    "                'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday',\n",
    "                'lane_count', 'wgs84_latitude', 'wgs84_longitude', 'morning_peak', 'evening_peak']\n",
    "feature_cols = numeric_meta + hour_cols + categorical_features\n",
    "\n",
    "# 4. Remove multicollinear features (threshold 0.95)\n",
    "corr_matrix = merged_data[feature_cols].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.95)]\n",
    "print(\"Dropping multicollinear features:\", to_drop)\n",
    "\n",
    "feature_cols = [f for f in feature_cols if f not in to_drop]\n",
    "\n",
    "# 5. Prepare dataset\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(merged_data[feature_cols], columns=categorical_features, drop_first=True)\n",
    "y = merged_data['congested']\n",
    "\n",
    "# 6. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Apply SMOTE to balance classes on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 8. Hyperparameter tuning for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "grid = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "                    param_grid, cv=3, scoring='f1', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train_res, y_train_res)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "print(\"Best XGBoost params:\", grid.best_params_)\n",
    "\n",
    "# 9. Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "report_dict = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"F1 Score\": f1_score(y_test, y_pred)\n",
    "}\n",
    "report_df = pd.DataFrame(report_dict, index=[\"Score\"]).T\n",
    "\n",
    "# 10. Output results\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(report_df)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Not Congested', 'Congested'],\n",
    "            yticklabels=['Not Congested', 'Congested'])\n",
    "plt.title(\"XGBoost Confusion Matrix After Improvements\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
