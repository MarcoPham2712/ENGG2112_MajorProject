{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87268a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial packages imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Data handling and numerical computations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning packages\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Machine learning models (Basic)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Machine learning models (Advanced)\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Real-time data and deployment (for future use)\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Ignoring warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "print(\"Initial packages imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6043ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged_data:\n",
      "['station_key', 'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq', 'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday', 'daily_total', 'hour_00', 'hour_01', 'hour_02', 'hour_03', 'hour_04', 'hour_05', 'hour_06', 'hour_07', 'hour_08', 'hour_09', 'hour_10', 'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16', 'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22', 'hour_23', 'road_functional_hierarchy', 'lane_count', 'road_classification_type', 'suburb', 'wgs84_latitude', 'wgs84_longitude']\n",
      "\n",
      "Are expected station metadata columns present?\n",
      "suburb: âœ…\n",
      "road_functional_hierarchy: âœ…\n",
      "road_classification_type: âœ…\n",
      "lane_count: âœ…\n",
      "wgs84_latitude: âœ…\n",
      "wgs84_longitude: âœ…\n",
      "\n",
      "Number of rows with completely missing station metadata: 0\n",
      "\n",
      "Sample merged rows with metadata:\n",
      "   station_key             suburb road_functional_hierarchy  \\\n",
      "0     15934005  Twelve Mile Creek              Primary Road   \n",
      "1     15934004  Twelve Mile Creek              Primary Road   \n",
      "2     15934005  Twelve Mile Creek              Primary Road   \n",
      "3        57052  Constitution Hill              Primary Road   \n",
      "4     15934004  Twelve Mile Creek              Primary Road   \n",
      "\n",
      "  road_classification_type      lane_count  wgs84_latitude  wgs84_longitude  \n",
      "0                  Highway  TwoOrMoreLanes      -32.656689       151.850708  \n",
      "1                  Highway  TwoOrMoreLanes      -32.656384       151.851227  \n",
      "2                  Highway  TwoOrMoreLanes      -32.656689       151.850708  \n",
      "3                    Drive  TwoOrMoreLanes      -33.797592       150.975433  \n",
      "4                  Highway  TwoOrMoreLanes      -32.656384       151.851227  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Select only essential metadata columns from the station file (with correct names)\n",
    "selected_station_cols = [\n",
    "    'station_key',\n",
    "    'suburb',\n",
    "    'road_functional_hierarchy',\n",
    "    'road_classification_type',\n",
    "    'lane_count',\n",
    "    'wgs84_latitude',\n",
    "    'wgs84_longitude'\n",
    "]\n",
    "\n",
    "\n",
    "# Load station metadata (small file, selective columns)\n",
    "station_data = pd.read_csv(\n",
    "    \"D:/study/ENGG2112/Major Project/drive-download-20250430T233625Z-001/trafficStations.csv\",\n",
    "    usecols=selected_station_cols\n",
    ")\n",
    "\n",
    "# Prepare for chunked loading\n",
    "chunks = []\n",
    "chunk_size = 25000  # adjust based on RAM\n",
    "\n",
    "# Stream traffic data in chunks\n",
    "for chunk in pd.read_csv(\n",
    "    \"D:/study/ENGG2112/Major Project/drive-download-20250430T233625Z-001/trafficData.csv\",\n",
    "    chunksize=chunk_size,\n",
    "    low_memory=True\n",
    "):\n",
    "    merged_chunk = pd.merge(chunk, station_data, on='station_key', how='left')\n",
    "    chunks.append(merged_chunk)\n",
    "\n",
    "# Combine chunks\n",
    "merged_data = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Convert core data types after merge to reduce memory\n",
    "column_types = {\n",
    "    'station_key': 'int32',\n",
    "    'traffic_direction_seq': 'int8',\n",
    "    'cardinal_direction_seq': 'int8',\n",
    "    'classification_seq': 'int8',\n",
    "    'year': 'int16',\n",
    "    'month': 'int8',\n",
    "    'day': 'int8',\n",
    "    'day_of_week': 'int8',\n",
    "    'public_holiday': 'bool',\n",
    "    'school_holiday': 'bool',\n",
    "    'daily_total': 'int32'\n",
    "}\n",
    "for i in range(24):\n",
    "    column_types[f'hour_{i:02d}'] = 'int16'\n",
    "\n",
    "merged_data = merged_data.astype({k: v for k, v in column_types.items() if k in merged_data.columns})\n",
    "\n",
    "# --- Sanity Check Section ---\n",
    "print(\"Columns in merged_data:\")\n",
    "print(merged_data.columns.tolist())\n",
    "\n",
    "expected_cols = selected_station_cols[1:]  # Skip station_key (already known to exist)\n",
    "\n",
    "print(\"\\nAre expected station metadata columns present?\")\n",
    "for col in expected_cols:\n",
    "    print(f\"{col}: {'âœ…' if col in merged_data.columns else 'âŒ'}\")\n",
    "\n",
    "null_check = merged_data[expected_cols].isnull().all(axis=1).sum()\n",
    "print(f\"\\nNumber of rows with completely missing station metadata: {null_check}\")\n",
    "\n",
    "print(\"\\nSample merged rows with metadata:\")\n",
    "print(merged_data[['station_key'] + expected_cols].dropna().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef663a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sanity check that all required columns exist in the dataset\n",
    "expected_columns = [\n",
    "    'station_key', 'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq',\n",
    "    'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday', 'daily_total'\n",
    "] + [f'hour_{i:02d}' for i in range(24)] + [\n",
    "    'road_functional_hierarchy', 'lane_count', 'road_classification_type', \n",
    "    'suburb', 'wgs84_latitude', 'wgs84_longitude'\n",
    "]\n",
    "\n",
    "# Try to list columns in merged_data (if loaded)\n",
    "try:\n",
    "    dataset_columns = merged_data.columns.tolist()\n",
    "    missing_columns = [col for col in expected_columns if col not in dataset_columns]\n",
    "except Exception:\n",
    "    missing_columns = expected_columns  # fallback\n",
    "\n",
    "# Step 2: Redefine component feature groups\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "categorical_features = ['road_functional_hierarchy', 'road_classification_type', 'suburb']\n",
    "numeric_meta = [\n",
    "    'traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq',\n",
    "    'year', 'month', 'day', 'day_of_week',\n",
    "    'public_holiday', 'school_holiday', 'lane_count',\n",
    "    'wgs84_latitude', 'wgs84_longitude'\n",
    "]\n",
    "\n",
    "# Step 3: Combine final feature set\n",
    "feature_cols = numeric_meta + hour_cols + categorical_features\n",
    "\n",
    "# Output result\n",
    "{\n",
    "    \"missing_columns_in_data\": missing_columns,\n",
    "    \"feature_cols_count\": len(feature_cols),\n",
    "    \"sample_feature_cols\": feature_cols[:5] + [\"...\"] + feature_cols[-5:]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert lane_count safely\n",
    "merged_data['lane_count'] = pd.to_numeric(merged_data['lane_count'], errors='coerce')\n",
    "\n",
    "# Step 2: Fill missing values intelligently\n",
    "# You can customize these defaults based on your domain knowledge\n",
    "merged_data['lane_count'] = merged_data['lane_count'].fillna(2)\n",
    "merged_data['wgs84_latitude'] = merged_data['wgs84_latitude'].fillna(merged_data['wgs84_latitude'].median())\n",
    "merged_data['wgs84_longitude'] = merged_data['wgs84_longitude'].fillna(merged_data['wgs84_longitude'].median())\n",
    "\n",
    "# Step 3: Confirm hour columns and fill any NaNs with 0 (assuming no traffic recorded)\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "merged_data[hour_cols] = merged_data[hour_cols].fillna(0)\n",
    "\n",
    "# Define binary target label before feature validation\n",
    "merged_data['congested'] = (merged_data['daily_total'] > 1000).astype(int)\n",
    "merged_data = merged_data.dropna(subset=['road_classification_type'])\n",
    "\n",
    "# Step 4: Confirm that all required columns are now clean\n",
    "columns_used = numeric_meta + hour_cols + categorical_features + ['congested']\n",
    "missing_check = merged_data[columns_used].isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Diagnostic print\n",
    "print(\"ðŸ” Columns with remaining NaNs (should all be 0):\")\n",
    "print(missing_check[missing_check > 0])\n",
    "\n",
    "# Now proceed safely\n",
    "X = merged_data[numeric_meta + hour_cols + categorical_features]\n",
    "y = merged_data['congested']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"âœ… Final shape of training data: {X_train.shape}, test data: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bfc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default LightGBM Model\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Train a basic LightGBM model (no balancing, no tuning)\n",
    "basic_lgb = lgb.LGBMClassifier(random_state=42)\n",
    "basic_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = basic_lgb.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Report results\n",
    "report_df = pd.DataFrame({\n",
    "    \"Score\": [acc, prec, rec, f1]\n",
    "}, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "\n",
    "print(\"\\n--- Basic LightGBM Classification Report ---\")\n",
    "print(report_df)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Not Congested', 'Congested'],\n",
    "            yticklabels=['Not Congested', 'Congested'])\n",
    "plt.title(\"Basic LightGBM Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76805d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# --- Assume merged_data is already loaded and cleaned, with column 'congested' defined ---\n",
    "\n",
    "# 1. Group infrequent suburbs into 'Other'\n",
    "suburb_counts = merged_data['suburb'].value_counts()\n",
    "common_suburbs = suburb_counts[suburb_counts > 50].index\n",
    "merged_data['suburb'] = merged_data['suburb'].apply(lambda x: x if x in common_suburbs else 'Other')\n",
    "\n",
    "# 2. Add peak-hour aggregates\n",
    "hour_cols = [f\"hour_{i:02d}\" for i in range(24)]\n",
    "merged_data['morning_peak'] = merged_data[[f'hour_{i:02d}' for i in range(7, 10)]].sum(axis=1)\n",
    "merged_data['evening_peak'] = merged_data[[f'hour_{i:02d}' for i in range(16, 19)]].sum(axis=1)\n",
    "\n",
    "# 3. Define feature columns\n",
    "categorical_features = ['road_functional_hierarchy', 'road_classification_type', 'suburb']\n",
    "numeric_meta = ['traffic_direction_seq', 'cardinal_direction_seq', 'classification_seq',\n",
    "                'year', 'month', 'day', 'day_of_week', 'public_holiday', 'school_holiday',\n",
    "                'lane_count', 'wgs84_latitude', 'wgs84_longitude', 'morning_peak', 'evening_peak']\n",
    "feature_cols = numeric_meta + hour_cols + categorical_features\n",
    "\n",
    "# 4. Remove multicollinear features (threshold 0.95)\n",
    "corr_matrix = merged_data[feature_cols].corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.95)]\n",
    "print(\"Dropping multicollinear features:\", to_drop)\n",
    "\n",
    "feature_cols = [f for f in feature_cols if f not in to_drop]\n",
    "\n",
    "# 5. Prepare dataset\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(merged_data[feature_cols], columns=categorical_features, drop_first=True)\n",
    "y = merged_data['congested']\n",
    "\n",
    "# 6. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Apply SMOTE to balance classes on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 1: Initialize LightGBM classifier\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Step 2: Train on SMOTE-resampled data\n",
    "lgb_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Step 3: Predict on test set\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 5: Create performance report DataFrame\n",
    "report_df = pd.DataFrame({\n",
    "    \"Score\": [acc, prec, rec, f1]\n",
    "}, index=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "\n",
    "# Step 6: Print report and plot confusion matrix\n",
    "print(\"\\n--- LightGBM Classification Report ---\")\n",
    "print(report_df)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=['Not Congested', 'Congested'],\n",
    "            yticklabels=['Not Congested', 'Congested'])\n",
    "plt.title(\"LightGBM Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
