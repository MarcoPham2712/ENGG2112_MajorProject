{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9325f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_23404\\4250243131.py:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
      "C:\\Users\\Danie\\AppData\\Local\\Temp\\ipykernel_23404\\4250243131.py:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m traffic_long[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m traffic_long[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create datetime for each row\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m traffic_long[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(traffic_long[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;241m+\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_timedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraffic_long\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhour\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m traffic_long\u001b[38;5;241m.\u001b[39mmerge(stations, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_key\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Create basic features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Danie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\tools\\timedeltas.py:201\u001b[0m, in \u001b[0;36mto_timedelta\u001b[1;34m(arg, unit, errors)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m--> 201\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCIndex):\n",
      "File \u001b[1;32mc:\\Users\\Danie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\tools\\timedeltas.py:266\u001b[0m, in \u001b[0;36m_convert_listlike\u001b[1;34m(arg, unit, errors, name)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arg\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     td64arr \u001b[38;5;241m=\u001b[39m \u001b[43msequence_to_td64ns\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Danie\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:1084\u001b[0m, in \u001b[0;36msequence_to_td64ns\u001b[1;34m(data, copy, unit, errors)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted to timedelta64[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[1;32m-> 1084\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1086\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load traffic and station data\n",
    "traffic = pd.read_csv('datasets_cleaned/trafficData.csv')\n",
    "stations = pd.read_csv('datasets_cleaned/trafficStations.csv')\n",
    "\n",
    "# Melt hourly traffic data\n",
    "hour_columns = [f'hour_{i:02d}' for i in range(24)]\n",
    "traffic_long = traffic.melt(\n",
    "    id_vars=['station_key', 'traffic_direction_seq', 'cardinal_direction_seq',\n",
    "             'classification_seq', 'year', 'month', 'day', 'day_of_week',\n",
    "             'public_holiday', 'school_holiday'],\n",
    "    value_vars=hour_columns,\n",
    "    var_name='hour',\n",
    "    value_name='traffic_count'\n",
    ")\n",
    "\n",
    "# Convert hour column from 'hour_00' to int\n",
    "traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
    "\n",
    "# Create datetime for each row\n",
    "traffic_long['datetime'] = pd.to_datetime(traffic_long[['year', 'month', 'day']]) + pd.to_timedelta(traffic_long['hour'], unit='h')\n",
    "\n",
    "df = traffic_long.merge(stations, on='station_key', how='left')\n",
    "\n",
    "# Create basic features\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Drop irrelevant text columns (optional)\n",
    "drop_cols = ['name', 'full_name', 'intersection']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['mab_way_type', 'road_functional_hierarchy', 'road_on_type',\n",
    "                    'lane_count', 'road_classification_type', 'rms_region',\n",
    "                    'lga', 'suburb']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['traffic_count', 'datetime', 'station_key'])  # Adjust as needed\n",
    "y = df['traffic_count']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3681cd",
   "metadata": {},
   "source": [
    "Training model for a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73122df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Quang Nguyen\\AppData\\Local\\Temp\\ipykernel_24548\\3054592484.py:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 8.11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first 50 rows of each file\n",
    "traffic = pd.read_csv(\"datasets_cleaned\\trafficData.csv\").head(500)\n",
    "stations = pd.read_csv(\"datasets_cleaned\\trafficStations.csv\")\n",
    "\n",
    "# Melt traffic data from wide to long format\n",
    "hour_columns = [f'hour_{i:02d}' for i in range(24)]\n",
    "traffic_long = traffic.melt(\n",
    "    id_vars=['station_key', 'traffic_direction_seq', 'cardinal_direction_seq',\n",
    "             'classification_seq', 'year', 'month', 'day', 'day_of_week',\n",
    "             'public_holiday', 'school_holiday'],\n",
    "    value_vars=hour_columns,\n",
    "    var_name='hour',\n",
    "    value_name='traffic_count'\n",
    ")\n",
    "\n",
    "# Convert hour to numeric\n",
    "traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
    "\n",
    "# Create datetime\n",
    "traffic_long['datetime'] = pd.to_datetime(traffic_long[['year', 'month', 'day']]) + pd.to_timedelta(traffic_long['hour'], unit='h')\n",
    "\n",
    "# Merge with station metadata\n",
    "df = traffic_long.merge(stations, on='station_key', how='left')\n",
    "\n",
    "# Add features\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Drop text columns (optional)\n",
    "drop_cols = ['name', 'full_name', 'intersection']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['mab_way_type', 'road_functional_hierarchy', 'road_on_type',\n",
    "                    'lane_count', 'road_classification_type', 'rms_region',\n",
    "                    'lga', 'suburb']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Limit to 50 rows again after transformation (optional for speed)\n",
    "df = df.head(500)\n",
    "\n",
    "# Split and train\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X = df.drop(columns=['traffic_count', 'datetime', 'station_key'], errors='ignore')\n",
    "y = df['traffic_count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7a880",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ac1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation RMSE Scores: [ 8.72494527 13.98610997 20.92654628 10.85487821  7.24731433]\n",
      "Average RMSE: 12.35\n",
      "Standard Deviation of RMSE: 4.85\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first 50 rows of each file\n",
    "traffic = pd.read_csv(\"datasets_cleaned\\trafficData.csv\").head(500)\n",
    "stations = pd.read_csv(\"datasets_cleaned\\trafficStations.csv\")\n",
    "\n",
    "# Melt traffic data from wide to long format\n",
    "hour_columns = [f'hour_{i:02d}' for i in range(24)]\n",
    "traffic_long = traffic.melt(\n",
    "    id_vars=['station_key', 'traffic_direction_seq', 'cardinal_direction_seq',\n",
    "             'classification_seq', 'year', 'month', 'day', 'day_of_week',\n",
    "             'public_holiday', 'school_holiday'],\n",
    "    value_vars=hour_columns,\n",
    "    var_name='hour',\n",
    "    value_name='traffic_count'\n",
    ")\n",
    "\n",
    "# Convert hour to numeric\n",
    "traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
    "\n",
    "# Create datetime\n",
    "traffic_long['datetime'] = pd.to_datetime(traffic_long[['year', 'month', 'day']]) + pd.to_timedelta(traffic_long['hour'], unit='h')\n",
    "\n",
    "# Merge with station metadata\n",
    "df = traffic_long.merge(stations, on='station_key', how='left')\n",
    "\n",
    "# Add features\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Drop text columns (optional)\n",
    "drop_cols = ['name', 'full_name', 'intersection']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['mab_way_type', 'road_functional_hierarchy', 'road_on_type',\n",
    "                    'lane_count', 'road_classification_type', 'rms_region',\n",
    "                    'lga', 'suburb']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Limit to 50 rows again after transformation (optional for speed)\n",
    "df = df.head(500)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Assume df is already prepared with features and target (traffic_count)\n",
    "\n",
    "X = df.drop(columns=['traffic_count', 'datetime', 'station_key'], errors='ignore')\n",
    "y = df['traffic_count']\n",
    "\n",
    "# Create the Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "# Let's use 5-folds (you can adjust k)\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Convert negative MSE to positive and calculate RMSE for each fold\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Cross-Validation RMSE Scores: {rmse_scores}\")\n",
    "print(f\"Average RMSE: {rmse_scores.mean():.2f}\")\n",
    "print(f\"Standard Deviation of RMSE: {rmse_scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c4050",
   "metadata": {},
   "source": [
    "Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a659e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m sss \u001b[38;5;241m=\u001b[39m StratifiedShuffleSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create the train-test split\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[43msss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraffic_bins\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     12\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[train_index], df\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Separate features (X) and target (y) for training and testing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Quang Nguyen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2405\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2401\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2403\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   2404\u001b[0m     )\n\u001b[1;32m-> 2405\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[1;32mc:\\Users\\Quang Nguyen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Quang Nguyen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:105\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first 50 rows of each file\n",
    "traffic = pd.read_csv(\"datasets_cleaned\\trafficData.csv\").head(500)\n",
    "stations = pd.read_csv(\"datasets_cleaned\\trafficStations.csv\")\n",
    "\n",
    "# Melt traffic data from wide to long format\n",
    "hour_columns = [f'hour_{i:02d}' for i in range(24)]\n",
    "traffic_long = traffic.melt(\n",
    "    id_vars=['station_key', 'traffic_direction_seq', 'cardinal_direction_seq',\n",
    "             'classification_seq', 'year', 'month', 'day', 'day_of_week',\n",
    "             'public_holiday', 'school_holiday'],\n",
    "    value_vars=hour_columns,\n",
    "    var_name='hour',\n",
    "    value_name='traffic_count'\n",
    ")\n",
    "\n",
    "# Convert hour to numeric\n",
    "traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
    "\n",
    "# Create datetime\n",
    "traffic_long['datetime'] = pd.to_datetime(traffic_long[['year', 'month', 'day']]) + pd.to_timedelta(traffic_long['hour'], unit='h')\n",
    "\n",
    "# Merge with station metadata\n",
    "df = traffic_long.merge(stations, on='station_key', how='left')\n",
    "\n",
    "# Add features\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Drop text columns (optional)\n",
    "drop_cols = ['name', 'full_name', 'intersection']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['mab_way_type', 'road_functional_hierarchy', 'road_on_type',\n",
    "                    'lane_count', 'road_classification_type', 'rms_region',\n",
    "                    'lga', 'suburb']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Limit to 50 rows again after transformation (optional for speed)\n",
    "df = df.head(500)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Binning the target variable (traffic counts) into categories\n",
    "# Let's categorize traffic counts into 3 bins: low, medium, and high\n",
    "traffic_bins = pd.cut(df['traffic_count'], bins=[0, 100, 500, float('inf')], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Stratified split using the bins\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the train-test split\n",
    "for train_index, test_index in sss.split(df, traffic_bins):\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "# Separate features (X) and target (y) for training and testing\n",
    "X_train = train_data.drop(columns=['traffic_count', 'datetime', 'station_key'], errors='ignore')\n",
    "y_train = train_data['traffic_count']\n",
    "\n",
    "X_test = test_data.drop(columns=['traffic_count', 'datetime', 'station_key'], errors='ignore')\n",
    "y_test = test_data['traffic_count']\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c64933",
   "metadata": {},
   "source": [
    "Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdea6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Quang Nguyen\\AppData\\Local\\Temp\\ipykernel_24548\\2363224174.py:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
      "C:\\Users\\Quang Nguyen\\AppData\\Local\\Temp\\ipykernel_24548\\2363224174.py:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m fold_rmse \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Cross-validation loop\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[43mskf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraffic_bins\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     train_data, test_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[train_index], df\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Features and target\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Quang Nguyen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:881\u001b[0m, in \u001b[0;36mStratifiedKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    878\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    880\u001b[0m     )\n\u001b[1;32m--> 881\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[1;32mc:\\Users\\Quang Nguyen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Quang Nguyen\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:105\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first 50 rows of each file\n",
    "traffic = pd.read_csv(\"datasets_cleaned\\trafficData.csv\").head(500)\n",
    "stations = pd.read_csv(\"datasets_cleaned\\trafficStations.csv\")\n",
    "\n",
    "# Melt traffic data from wide to long format\n",
    "hour_columns = [f'hour_{i:02d}' for i in range(24)]\n",
    "traffic_long = traffic.melt(\n",
    "    id_vars=['station_key', 'traffic_direction_seq', 'cardinal_direction_seq',\n",
    "             'classification_seq', 'year', 'month', 'day', 'day_of_week',\n",
    "             'public_holiday', 'school_holiday'],\n",
    "    value_vars=hour_columns,\n",
    "    var_name='hour',\n",
    "    value_name='traffic_count'\n",
    ")\n",
    "\n",
    "# Convert hour to numeric\n",
    "traffic_long['hour'] = traffic_long['hour'].str.extract('hour_(\\d+)').astype(int)\n",
    "\n",
    "# Create datetime\n",
    "traffic_long['datetime'] = pd.to_datetime(traffic_long[['year', 'month', 'day']]) + pd.to_timedelta(traffic_long['hour'], unit='h')\n",
    "\n",
    "# Merge with station metadata\n",
    "df = traffic_long.merge(stations, on='station_key', how='left')\n",
    "\n",
    "# Add features\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Drop text columns (optional)\n",
    "drop_cols = ['name', 'full_name', 'intersection']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['mab_way_type', 'road_functional_hierarchy', 'road_on_type',\n",
    "                    'lane_count', 'road_classification_type', 'rms_region',\n",
    "                    'lga', 'suburb']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Limit to 50 rows again after transformation (optional for speed)\n",
    "df = df.head(500)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Binning the target variable (traffic counts) into categories\n",
    "traffic_bins = pd.cut(df['traffic_count'], bins=[0, 100, 500, float('inf')], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Create Stratified K-Fold cross-validation (e.g., 5 folds)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Store RMSE for each fold\n",
    "fold_rmse = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in skf.split(df, traffic_bins):\n",
    "    # Split data\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    # Features and target\n",
    "    X_train = train_data.drop(columns=['traffic_count', 'datetime', 'station_key'], errors='ignore')\n",
    "    y_train = train_data['traffic_count']\n",
    "    \n",
    "    X_test = test_data.drop(columns=['traffic_count', 'datetime', 'station_key'], errors='ignore')\n",
    "    y_test = test_data['traffic_count']\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and calculate RMSE for this fold\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    fold_rmse.append(rmse)\n",
    "\n",
    "# Print the RMSE for each fold and average RMSE\n",
    "print(f\"Fold-wise RMSE: {fold_rmse}\")\n",
    "print(f\"Average RMSE: {np.mean(fold_rmse):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeebc50",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
